System startup sequence
~~~~~~~~~~~~~~~~~~~~~~~
This project assumes/implements/is based around the idea of "staged init",
which means that during system operations, several distinct executables
run as pid 1, exec'ing sequentially into each other. Typical chain of pid 1
executables looks like this:

	/linuxrc              # msh script
	 |
	 v
	switchroot
	 |
	 v
	/etc/boot/start       # msh script
	 |
	 v
	svhub
	 |
	 v
	/etc/boot/reboot      # msh script
	 |
	 v
	reboot

Startup and shutdown stages (everything above and below svhub) are for the most
part strictly sequential, doing one thing at a time. When it gets to svhub,
startup become strictly parallel, as svhub spawns everything at once. During
shutdown, svhub kills all its children and execs into another sequential stage
which eventually bring the system down.


Initrd and the early startup
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Locating and mounting the root filesystem in Linux is a non-trivial task.
The idea behind intird is to let the bootloader load a simple filesystem
image into RAM image for the kernel to use, pass the control to the early
userspace code from initrd, and do the rest of the work there.

# Technical note: modern kernels typically do initramfs instead of initrd,
# and in fact this project only supports initramfs at this moment. However,
# the distinction is not really important outside of the kernel, so we will
# use "initrd" to refer to either of them.

The kernel "mounts" initrd as / and spawns a single process, typically
/init or /linuxrc, which should locate and/or wait for the root device,
mount it, and pass the control there. Typical implementation:

    #!/bin/msh

    mount -v /dev /sys /proc

    devinit # creates /dev/mapper/root

    kmount /root /dev/mapper/root

    exec switchroot /root /etc/boot/start

The overall goal of this script is simply to get the real root mounted
somewhere, and then call switchroot to that directory. The `devinit`
part is where most of problems are though.


Locating the root device
~~~~~~~~~~~~~~~~~~~~~~~~
There are essentially three problems to solve at this stage: load driver
modules for the storage device and the bus it's on; wait for the device to
be initialized; and finally, since Linux gives uniform names to device nodes
(like /dev/sdX), figure out which node corresponds to the right device.

The tools that address them are `devinit` and `findblk`. `devinit` spawns
a script, and handles module-related udev events while the script is running.
The script, /etc/devinit, should then spawn `findblk` which will probe all
block devices appearing in /dev until it finds the one it needs. Once device
has been located, `findblk` symlinks partitions to /dev/mapper and exits.

Typical contents of /etc/devinit:

    #!/bin/msh

    findblk mbr:00112233 1:boot 2:root

See man pages for `devinit` and `findblk` for detailed description.

Note that to work properly, `devinit` needs one more script, /etc/modpipe:

    #!/bin/msh

    exec modprobe -p

See modprobe man page on what it does.


Statically-located root devices
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In case the kernel is module but the list of module to load is known ahead
of time, `devinit` should be skipped, and both `modprobe` and `findblk` should
be called directly from /linuxrc:

    #!/bin/msh

    mount -vc /dev /sys /proc

    modprobe some-mmc-controller

    findblk name:mmcblk0 p1:root

    kmount /root /dev/mapper/root

    exec switchroot /root /etc/boot/start

The reason to use `findblk` in the example above is to pause the script
until /dev/mmcblk0 appears, since modprobe does not wait for the module
to initialize. In simpler cases, it can be replaced with `waitfor` msh
built-in.

If the name of the device node is static, and it does not require waiting,
`findblk` stage can be skipped as well:

    #!/bin/msh

    mount -vc /dev /sys /proc

    kmount /root /dev/mmcblk0p1

    exec switchroot /root /etc/boot/start

At this point, the need to use initrd in the first place should be questioned.


Encrypted rootfs
~~~~~~~~~~~~~~~~
Encryption layer must be setup up before mounting the file system.
Sample /etc/devinit using `passblk`:

    #!/bin/msh

    findblk mbr:00112233 1:root

    passblk /etc/keyfile root

See `passblk` man page for a detailed description. The end result, again, will
be a mountable partition pointed to by /dev/mapper/root.

Running `passblk` from devinit given the kernel a chance to request encryption
support modules if those aren't loaded. Alternatively, the module can be loaded
statically in /linuxrc, and `findblk` can be called there as well:

    #!/bin/msh

    mount -vc /dev /sys /proc

    modprobe crypto_engine
    modprobe crypto_user
    modprobe cryptd
    modprobe cbc
    modprobe ecb
    modprobe xts
    modprobe dm-crypt

    devinit

    passblk /etc/keyfile root

    kmount /root /dev/mapper/root

    exec switchroot /root /etc/boot/start

Depending on cofiguration, some module may need to be loaded statically even
if `passblk` gets called from /etc/devinit.


Entering to the real root
~~~~~~~~~~~~~~~~~~~~~~~~~
Once the root partition is mounted, /linuxrc should exec into `switchroot`,
which will subsequently exec into the next stage, which in the examples above
is `/etc/boot/start`. Not much else to discuss here, check the man page for
`switchroot` for detailed description of what exactly it does.

Note this is the one of the few points outside of the kernel where the
distinction between initrd proper and initramfs matters. `switchroot` only
works with initramfs. Switching from initrd proper requires a different tool
that would perform a different syscall sequence. There is no such tool in this
project at this moment. Initrd proper (as in, initial ramdisk) is generally
considered deprecated in favor of initramfs.


Virtual and non-root filesystems
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
It is up to the system builder to decide whether to mount particular virtual
filesystems in initrd, or delay it to the real root startup script. There are
several that need to be mounted early: /dev, /sys and possibly /proc. This
depends solely on the tools used in initrd. For instance, `findblk` needs both
/dev and /sys to work.

In case separate partitions are used for /var or some other directories,
those probably should be located and possibly mounted from initrd as well.
This mostly depends on the convenience of running devinit (and/or findblk)
from the real root.

`switchroot` will attempt to preserve any mount points when switching from
the virtual initramfs root to the real filesystem. Check the man page on how
it works.


Bulk storage and userspace-managed filesystems
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
The suggested way to handle any filesystem that cannot be mounted with
a simple kmount is not to use it for the system itself, and not to mount
it during sequential startup at all.

Use a simple, easily-mountable, possibly read-only filesystem (on a separate
device if necessary) to hold the base OS, spawn userspace managers as regular
services, and do not depend on them being available during startup.

if dedicated simple rootfs is out of question, it is up to the system builder
to figure out how to boot the system in a reliable manner. This will likely
require writing a number of rather tricky tools.

XFS, ZFS, NFS, SMB, LVM with any payload, FUSE-anything require at least some
userspace support. Pretty much anything networking, hot-swappable, or capable
of using multiple volumes.

Note that late mounting by the managing service means the applications using
those filesystems must be able start and run without their storage available.
For instance, httpd serving files from a large XFS tree should be able to start
successfully and keep returning 404 for any request if the XFS mount point is
empty.


Real root startup script
~~~~~~~~~~~~~~~~~~~~~~~~
After completing its job, `switchroot` execs into a script from the real root.
At this point, initrd is gone, the root filesystem is in place ready to use,
but there are no processes running in the system yet other than the script
itself.

The role of this startup script is to complete the sequential part of system
initialization, and get it ready to run svhub and proceed with the parallel
part of the sequence.

Depending on system setup, this means mounting remaining virtual filesystems
if any, and setting up various other aspects of the global kernel state.
Some static module may need to be loaded, maybe initial (non-device-dependent)
iptables set up, hostname, maybe even console fonts and so on. Any kind of
system state that needs to be set up once. Quick example, /etc/boot/start:

    #!/bin/msh

    setenv PATH /bin:/sbin:/usr/bin
    
    /sbin/kmount -vc /dev/pts
    /sbin/kmount -vc /run
    /sbin/kmount -vc /mnt
    /sbin/kmount -vc /tmp
    # remount root read-write
    /sbin/kmount -e /
    
    umask 0002
    
    mkdir /run/ctrl
    
    prctl no-new-privs
    
    exec /sbin/svhub

Keep in mind this part is very system-dependent, other than the final exec.


Fan-out and steady-state operations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Sequential startup stage ends with the startup script exec'ing into svhub.
By this point, the kernel state has been set up and the system is ready
to run regular services.

Up until this point, the number of processes in the system should have been
pretty low, the process graph mostly linear, and the processes involved were
short-running. At this point, the process graph fans out, with multiple
processes spawning simultaneously, and these processes are now long-lived.
They will keep running for the majority of the time the system is up,
monitored by svhub. This is the steady state of the system.

Note the role of svhub is to start the processes that should be started
unconditionally, every time the system boots. Anything that should be started
in response to some event, should be started in some other way. The only event
svhub handles is the system startup.

Another important point regarding svhub is that it starts all its children
simultaneously and they run concurrently. Whatever things they do, they
should be ready to work independently from each other.

Check svhub man pages, and ./services.txt here for more information on setting
up long-running services.


Shutdown sequence
~~~~~~~~~~~~~~~~~
svhub normally does not exit on its own (and neither should the services it
runs). It can be commanded to exit, in which case it kills all its children,
waits for them to dies, and execs into one of the shutdown scripts depending
on which command it's gotten:

    /etc/boot/reboot
    /etc/boot/shutdown
    /etc/boot/powerdown

There may be system-dependent things there but eventually all three should
exec into `reboot` with appropriate flags. `reboot` will (attempt to) un-mount
all remaining filesystems and tell the kernel to reset the system.

By the time any of the shutdown scripts run, all processes that could be
stopped would have been stopped (by svhub or one of its children), and the
system would be back to sequential operation.
